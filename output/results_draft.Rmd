---
title: "Results Draft"
author: "Michael Juberg & Polina Beloborodova"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2:
    number_sections: false
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(knitr)
library(ggplot2)
library(tidyverse)
library(rstatix)
library(psych)
library(corrr)
library(qgraph)
library(bootnet)
library(EstimateGroupNetwork)
library(NetworkComparisonTest)
library(networktools)

load("data/clean/selfcomp_clean_means.Rda")
load("data/clean/selfcomp_demographic.Rda")
load("data/clean/selfcomp_all.Rda")

sc <- self_comp_demographic

sc <-
  sc %>%
  mutate(group = ifelse(semester %in% c("Spring2019","Fall2019"), "PreCOVID",
                        "PostCOVID"))

sc_net <-
  self_comp_clean_means %>%
  select(sdo_mean_win,
         iri_ec_pt_mean_win,
         scs_selfkindness_mean,
         scs_selfjudgment_mean,
         scs_commonhuman_mean,
         scs_isolation_mean,
         scs_mindfulness_mean,
         scs_overidentified_mean,
         group)

vars <- colnames(self_comp)

sdo_items <- vars[grep("^sdo\\d{1,2}$", vars)]
scs_items <- vars[grep("^scs\\d{1,2}$", vars)]
iri_items <- vars[grep("^iri\\d{1,2}$", vars)]

```

## Method

### Participants

The data were collected from `r nrow(sc)` students of the University of Hawaiʻi at Mānoa. Participants' age ranged from `r min(sc$age)` to `r max(sc$age)` (*M* = `r round(mean(sc$age), 2)`, *SD* = `r round(sd(sc$age), 2)`). Participants self-identified as female (`r round(100*sum(sc$gender == "Female")/nrow(sc), 2)`%), male (`r round(100*sum(sc$gender == "Male")/nrow(sc), 2)`%), and other gender (`r sum(sc$gender == "Other")` persons). Self-reported ethnicity was distributed as follows: White (`r round(100*sum(sc$ethnicity_prescreen == "White (European American, Caucasian, Anglo, etc.)", na.rm = TRUE)/nrow(sc), 2)`%), East Asian (`r round(100*sum(sc$ethnicity_prescreen == "East Asian (Chinese, Korean, Japanese, etc.)", na.rm = TRUE)/nrow(sc), 2)`%), Bi- or multi-ethnic/racial (`r round(100*sum(sc$ethnicity_prescreen %in% c("Bi- or multi-ethnic (White/Black, White/East Asian, etc.)", "Bi- or multi-racial (White/Black, White/East Asian, etc.)"))/nrow(sc), 2)`%), Southeast Asian (`r round(100*sum(sc$ethnicity_prescreen == "Southeast Asian (Vietnamese, Cambodian, Filipino, etc.)", na.rm = TRUE)/nrow(sc), 2)`%), Latina/o (`r round(100*sum(sc$ethnicity_prescreen == "Latina/o (Hispanic American, Colombian, Mexican, etc.)", na.rm = TRUE)/nrow(sc), 2)`%), Native Hawaiian (`r round(100*sum(sc$ethnicity_prescreen == "Native Hawaiian", na.rm = TRUE)/nrow(sc), 2)`%), Other Polynesian or Pacific Islander (`r round(100*sum(sc$ethnicity_prescreen == "Other Polynesian or Pacific Islander (Micronesian, Melanesian, Samoan, etc.)", na.rm = TRUE)/nrow(sc), 2)`%), Black (`r round(100*sum(sc$ethnicity_prescreen == "Black (African American, African, Caribbean, etc.)", na.rm = TRUE)/nrow(sc), 2)`%), American Indian, Alaskan Native, or other Aboriginal (`r sum(sc$ethnicity_prescreen == "American Indian, Alaskan Native, or other Aboriginal (Cherokee, Navajo, Sioux, etc.)", na.rm = TRUE)` persons), South Asian (`r sum(sc$ethnicity_prescreen == "South Asian (Pakistani, Indian, Sri Lankan, etc.)", na.rm = TRUE)` persons), and other (`r sum(sc$ethnicity_prescreen == "Other not listed", na.rm = TRUE)` person); `r sum(is.na(sc$ethnicity_prescreen))` participants did not report their ethnicity.

### Procedure

The participants were recruited via SONA system technology, a web-based software through which students can participate in online studies and earn extra course credit. Only 18 or more years old undergraduate students could participate. After completing the consent form, they proceeded to the questionnaire. The participants could not skip questions. They could stop their participation and erase their data at any moment before submitting their answers. After completing the survey, the participants could retrieve their extra credit for a class through SONA system. The data were collected in two waves: before the COVID-19 pandemic (Spring and Fall semesters, 2019, *N* = `r sum(sc$semester == "Spring2019") + sum(sc$semester == "Fall2019")`) and during it (Fall semester, 2020, *N* = `r sum(sc$semester == "Fall2020")`).

```{r demographics-prepost}

wilcox_age <- wilcox.test(age ~ group, data = sc)
zstat_age <- qnorm(wilcox_age$p.value/2)
d_age <- zstat_age/sqrt(nrow(sc))

chisq_gender <- 
  sc %>%
  filter(gender != "Other") %>%
  select(group, gender) %>%
  count(group, gender) %>%
  pivot_wider(names_from = gender, values_from = n) %>%
  select(-1) %>%
  chisq_test()

fisher_ethnic <- 
  sc %>%
  select(group, ethnicity_prescreen) %>%
  drop_na(ethnicity_prescreen) %>%
  filter(!(ethnicity_prescreen %in% c("Other not listed", "South Asian (Pakistani, Indian, Sri Lankan, etc.)"))) %>%
  mutate(ethnicity_prescreen = if_else(ethnicity_prescreen == "Bi- or multi-racial (White/Black, White/East Asian, etc.)", "Bi- or multi-ethnic (White/Black, White/East Asian, etc.)", ethnicity_prescreen)) %>%
  count(group, ethnicity_prescreen) %>%
  pivot_wider(names_from = ethnicity_prescreen, values_from = n) %>%
  select(-1) %>%
  fisher_test(simulate.p.value = TRUE)

```

Pre- and post-COVID groups were demographically equivalent. There were no significant differences in the gender ratio, according to the Chi-squared Test  (chi-square = `r round(chisq_gender$statistic, 2)`, *df* = `r chisq_gender$df`, *p* = `r round(chisq_gender$p, 3)`; participants who identified as 'other gender' were excluded from comparison due to the small group size). Proportions of ethnicities were not significantly different either according to the Fisher’s exact test with Monte Carlo simulation (*p* = `r round(fisher_ethnic$p, 3)`). The groups significantly differed by age, according to the Wilcoxon Signed-Rank Test, *p* = `r round(wilcox_age$p.value, 3)`, Cohen’s *d* = `r round(d_age, 2)` (*M* = `r round(mean(subset(sc, group == "PreCOVID")$age), 2)` vs. *M* = `r round(mean(subset(sc, group == "PostCOVID")$age), 2)` in the pre- and post-COVID groups respectively). However, the difference was less than one standard deviation in the total sample. 

### Measures

#### Self-Compassion ####

Self-compassion was assessed using the Self-Compassion Scale (SCS; Neff, 2003a). The SCS is a 26-item self-report measure and consists of six subscales that measure three components of self-compassion (Neff, 2003a; 2003b). These components consist of opposing pairs: self-kindness (e.g., “I’m kind to myself when I’m experiencing suffering”) vs. self-judgment (e.g., “I’m disapproving and judgmental about my own flaws and inadequacies”); common humanity (e.g., “I try to see my failings as part of the human condition”) vs. isolation (e.g., “When I fail at something that's important to me, I tend to feel alone in my failure”); and mindfulness (e.g., “When I'm feeling down I try to approach my feelings with curiosity and openness”) vs. over-identification (e.g., “When something upsets me I get carried away with my feelings”). Participants respond on a 5-point Likert scale ranging from 1 (*strongly disagree*) to 5 (*strongly agree*). Subscales scores are computed by calculating the mean of the subscale item responses. High scores reflect greater levels of the facets of self-compassion. The scale demonstrated good internal consistency in previous studies, with Cronbach’s alpha from .92 to .94 (Neff, 2003a; Neff, Kirkpatrick et al., 2007). The estimated internal consistency in current study was as follows: overall scale (`r round(psych::alpha(self_comp[,scs_items])$total$raw_alpha, 2)`), self-kindness subscale (`r round(psych::alpha(self_comp[,scs_items[c(5,12,19,23,26)]])$total$raw_alpha, 2)`), self-judgment subscale (`r round(psych::alpha(self_comp[,scs_items[c(1,8,11,16,21)]])$total$raw_alpha, 2)`), common humanity subscale (`r round(psych::alpha(self_comp[,scs_items[c(3,7,10,15)]])$total$raw_alpha, 2)`), isolation subscale (`r round(psych::alpha(self_comp[,scs_items[c(4,13,18,25)]])$total$raw_alpha, 2)`), mindfulness subscale (`r round(psych::alpha(self_comp[,scs_items[c(9,14,17,22)]])$total$raw_alpha, 2)`), over-identification subscale (`r round(psych::alpha(self_comp[,scs_items[c(2,6,20,24)]])$total$raw_alpha, 2)`).

#### Empathy ####

Empathy was assessed using the Interpersonal Reactivity Index (IRI; Davis, 1980). This multidimensional measure consists of four subscales: perspective-taking, empathic concern, personal distress, and fantasy. The full measure contains 28 items using a 5-point Likert scale with responses ranging from 0 (*does not describe me well*) to 4 (*describes me very well*). Sample items include “When I see someone being taken advantage of, I feel kind of protective towards them”; “When I see someone get hurt, I tend to remain calm”; and “Before criticizing somebody, I try to imagine how I would feel if I were in their place.” Items for subscales are averaged. Higher scores indicate greater facets of empathy. The IRI demonstrated adequate internal consistency in previous research with Cronbach’s alpha from .70 to .78 (Davis, 1980). In later research empathy was increasingly measured using a composite of empathic concern and perspective-taking subscales (see Duriez, 2004; McFarland, 2010). In the present study, we also used the average of empathic concern and perspective-taking subscales items, with higher scores indicating increased levels of empathy. The composite of empathic concern and perspective-taking had a Cronbach’s alpha of `r round(psych::alpha(self_comp[,iri_items[c(2,4,9,14,18,20,22,3,8,11,15,21,25,28)]])$total$raw_alpha, 2)` in the current study.

#### Social Dominance Orientation####

Social Dominance Orientation (SDO-6; Pratto et al., 1994) is a self-report measure that assesses one’s degree of preference for inequality among social groups. The SDO measure asks questions such as “Which of the following objects or statements do you have a negative or positive feeling towards?” The 16-item scale is measured on a 7-point Likert scale which represents the degree of positive and negative feeling to each statement, ranging from 1 (*very negative*) to 7 (*very positive*). Sample items include: “Some groups of people are simply inferior to others;” “All groups should be given an equal chance in life;” and “No one group should dominate in society.” Items are averaged for a total score. High scores indicate a greeater preference for social inequality between groups and lower scores indicate a preference for egalitarian worldviews. SDO exhibited high levels of internal consistency in previous research, with Cronbach’s alpha of .91; (Pratto et al., 1994). In the present study, Cronbach’s alpha of SDO was `r round(psych::alpha(self_comp[,sdo_items])$total$raw_alpha, 2)`.

### Statistical Analyses

We used psychometric network analysis in this study. This approach is extensively used in other fields, but it is relatively new in psychology. It allows to create a visual representation of complex associations among variables and find central variables which exert greater influence on the network. We conducted the analysis in three steps. First, we estimated the networks for the pre- and post-COVID samples and calculated their centrality indexes. Then, we estimated stability of the networks. Finally, we compared the networks on several aspects. All analyses were carried out in R version 4.0.2 in R-Studio 1.3.1073.

In network analysis, variables are called ‘nodes’, and connections between them are called ‘edges.’ In our analysis, we used the Gaussian Graphical Model (GGM; Lauritzen, 1996), a network in which nodes represented psychological constructs (social dominance orientation, empathy, and facets of self-compassion, calculated as the means of the items), and edges represented their partial correlations. If two constructs are linked in the resulting graph, they are dependent after controlling for all other constructs. If there is no link, they are thought to be independent. To determine an optimal network, we used the Fused Graphical Lasso (FGL; Costantini et al., 2019; Danaher et al., 2014), a method that applies regularization to avoid estimating spurious edges across multiple networks jointly. FGL applies a penalty on network density (the number and strength of edges) and another penalty on differences among corresponding edge weights in networks estimated on different samples. In sum, this method allows networks to share common structures while preserving the differences. Tuning parameters that regulate those penalty terms were selected via Extended Bayesian Information Criterion (EBIC) using *EstimateGroupNetwork* R-package (Costantini et al., 2019). Networks were visualized with *qgraph* R-package (Epskamp et al., 2012).

We calculated the following centrality indexes for each node in the two jointly estimated networks using *qgraph* R-package (Epskamp et al., 2012): strength (sum of absolute edge weights connected to a node), closeness (the inverse of the sum of the distance of the node to all other nodes), betweenness (the number of times when a node is on the shortest way between other nodes), and expected influence (sum of non-absolute edge weights connected to a node).

Stability of the networks was estimated via *bootnet* R-package (Epskamp et al., 2018). Stability estimation has not yet been implemented for jointly estimated networks, hence following the example of Fried et al. (2017), we evaluated stability of pre- and post-COVID networks estimated separately with a penalty only on network density. Since independently estimated networks employ samples separately, they are less powered and stable than jointly estimated networks. Hence, stability of independently estimated networks represents the lower bound of stability of jointly estimated networks. To evaluate stability of the edge-weights, we calculated their 95% confidence intervals with 5,000 nonparametric bootstraps. Stability of centrality metrics was assessed with 5,000 case-dropping bootstraps used to calculate the Correlation Stability (CS) coefficient that shows the largest proportion of the sample that can be dropped while maintaining a correlation of .70 between the original centrality indexes and those obtained from bootstrapped subsets. Values above 0.25 indicate moderate, above 0.5 strong stability (Epskamp et al., 2018).

Finally, we compared pre- and post-COVID networks. First, to assess their similarity, we calculated a correlation coefficient for the edge-weigths in the two networks (Borsboom et al., 2017; Rhemtulla et al., 2016). Then, we conducted Network Comparison Test (NCT), implemented in *NetworkComparisonTest* R package (van Borkulo et al., 2020). NCT is a permutation-based hypothesis test that assesses the difference between two networks using several invariance measures. We started the analysis with an omnibus test that showed whether all edges were exactly the same. We also tested invariance of global strength (the sum of absolute edge-weights in each network), as well as the strength and expected influence of individual nodes. As a final step, we visualized the cross-sample network with edge-weights averaged between pre- and post-COVID samples.

### Data Pre-treatment

We used the following procedures to ensure data quality. First, as per Greszki et al. (2015) recommendations, we removed the data of the participants who completed the survey twice faster than the median completion time (*N* = 44, 4.26% of the sample). Next, we checked for univariate and multivariate outliers. Two univariate outliers (z-scores outside of ±3.29 range, *p* < .001; Tabachnick & Fidell, 2013) were detected and Winsorized (replaced with a value corresponding to the z-score of 3.29). We also removed four multivariate outliers, resulting in the final sample of 986 participants (*N* = 550 in the pre-COVID sample and *N* = 436 in the post-COVID sample). Skewness and kurtosis of all variables were within ±1 range (see Table \@ref(tab:descriptives)).

## Results

### Preliminary Analyses

#### Descriptive statistics

As a preliminary step, we calculated descriptive statistics for all study variables (see Table \@ref(tab:descriptives)). There was substantial variability in the data: participants showed a wide range of scores, from smallest to largest on all scales.

```{r descriptives}

  sc_net %>%
  select(-group) %>%
  psych::describe() %>%
  tibble() %>%
  mutate(variable = c("Social Dominance Orientation",
                      "IRI EC/PT Composite",
                      "SCS Self-Kindness",
                      "SCS Self-Judgment",
                      "SCS Common Humanity",
                      "SCS Isolation",
                      "SCS Mindfulness",
                      "SCS Over-Identification")) %>%
  select(variable, mean, sd, min, max, skew, kurtosis) %>%
  kable(caption = "Descriptive Statistics of Study Varibles",
        col.names = c("Variable", "Mean", "SD", "Min", "Max", "Skewness", "Kurtosis"),
        digits = 2)

```

Mean levels of the study variables were not significantly different in the pre- and post-COVID groups, with an exception of the Over-Identification subscale of the Self-Compassion Scale (see Table \@ref(tab:psych-prepost)). Participants in the post-COVID group were more identified with their thoughts and feelings than those in the pre-COVID group (*M* = `r round(mean(subset(sc_net, group == "PreCOVID")$scs_overidentified_mean), 2)` vs. *M* = `r round(mean(subset(sc_net, group == "PostCOVID")$scs_overidentified_mean), 2)`).

``` {r psych-prepost}

self_comp_clean_means %>%
  select(group, sdo_mean_win, iri_ec_pt_mean_win, starts_with("scs"), -scs_mean) %>%
  pivot_longer(-group, names_to = "variables", values_to = "value") %>%
  group_by(variables) %>%
  t_test(value ~ group) %>%
  adjust_pvalue(method = "holm") %>%
  add_significance() %>%
  select("variables", "statistic", "df", "p.adj") %>%
  mutate(variables = c("2. IRI EC/PT Composite",
                        "5. SCS Common Humanity",
                        "6. SCS Isolation",
                        "7. SCS Mindfulness",
                        "8. SCS Over-Identification",
                        "4. SCS Self-Judgment",
                        "3. SCS Self-Kindness",
                        "1. Social Dominance Orientation")) %>%
  arrange(variables) %>%
  kable(caption = "Pre-post T-tests for Means of Study Variables",
        col.names = c("Variable", "Statistic", "DF", "P-value (Holm-adjusted)"),
        digits = 2)

```

#### Bivariate Correlations

We calculated bivariate Pearson correlations coefficients for all study variables. Consistently with previous research, SDO was moderately negatively related to empathy (*p* < .001). Additionally, it showed a weak negative correlation with SCS Isolation subscale (*p* = .007). Empathy was weakly to moderately related to all "positive" SCS subscales: Self-Kindness, Common Humanity, and Mindfulness (*p*'s < .001). All SCS subscales showed significant correlations with each other (see Table \@ref(tab:cor-table)).

```{r cor-table}

sc_net %>%
  select(-group) %>%
  cor() %>%
  formatC(2, format = "f") %>%
  replace(upper.tri(.), " ") %>%
  data.frame() %>%
  remove_rownames() %>%
  rename_all(list(~ as.character(c(1:8)))) %>%
  mutate(Variable = c("1. Social Dominance Orientation",
                      "2. IRI EC/PT Composite",
                      "3. SCS Self-Kindness",
                      "4. SCS Self-Judgment",
                      "5. SCS Common Humanity",
                      "6. SCS Isolation",
                      "7. SCS Mindfulness",
                      "8. SCS Over-Identification")) %>%
  relocate(Variable) %>%
  kable(caption = "Bivariate Correlations between Study Varibles")

```

P-values (auxiliary, will not go into the paper)

```{r sig-table}

sc_net %>%
  select(-group) %>%
  cor_pmat %>%
  select(-rowname) %>%
  mutate_all(as.numeric) %>%
  mutate_all(round, 3) %>%
  rename_all(list(~ as.character(c(1:8)))) %>%
  mutate(Variable = c("1. Social Dominance Orientation",
                      "2. IRI EC/PT Composite",
                      "3. SCS Self-Kindness",
                      "4. SCS Self-Judgment",
                      "5. SCS Common Humanity",
                      "6. SCS Isolation",
                      "7. SCS Mindfulness",
                      "8. SCS Over-Identification")) %>%
  relocate(Variable) %>%
  kable(label = NA, digits = 3)

```


### Network Analysis

#### Network Estimation

```{r estimate-networks}

colnames(sc_net) <- c("SDO",
                      "IRI-EC/PT",
                      "SCS-SK",
                      "SCS-SJ",
                      "SCS-CH",
                      "SCS-I",
                      "SCS-M",
                      "SCS-OI",
                      "group")

net_pre <- estimateNetwork(sc_net[sc_net$group == "PreCOVID",-9],
                             default = "EBICglasso", 
                             weighted = TRUE,
                             threshold = TRUE, 
                             corMethod = "cor_auto")

net_post <- estimateNetwork(sc_net[sc_net$group == "PostCOVID",-9],
                             default = "EBICglasso",
                             weighted = TRUE,
                             threshold = TRUE,
                             corMethod = "cor_auto")

L <- averageLayout(net_pre, net_post)

net_joint <- EstimateGroupNetwork(sc_net,
                                  inputType = "dataframe",
                                  groupID = "group",
                                  method = "InformationCriterion",
                                  criterion = "ebic",
                                  strategy = "sequential",
                                  covfun = cor_auto,
                                  simplifyOutput = FALSE,
                                  seed = 23)

```

Jointly estimatуd pre- and post-COVID networks are demonstrated in Figure \@ref(fig:net-joint). A description of the node labels can be seen in Table \@ref(tab:node-labels). Pre-COVID network contained `r sum(net_joint$network[[1]][upper.tri(net_joint$network[[1]])] != 0)` edges (out of possible `r 8*(8-1)/2`), `r sum(net_joint$network[[1]][upper.tri(net_joint$network[[1]])] > 0)` of them positive and `r sum(net_joint$network[[1]][upper.tri(net_joint$network[[1]])] < 0)` negative. Post-COVID network also contained `r sum(net_joint$network[[2]][upper.tri(net_joint$network[[2]])] != 0)` edges, `r sum(net_joint$network[[2]][upper.tri(net_joint$network[[2]])] > 0)` of them positive and `r sum(net_joint$network[[2]][upper.tri(net_joint$network[[2]])] < 0)` negative. Network structures were very similar. In both of them, SDO had the strongest edge only with empathy, which in turn was connected to several nodes in the self-compassion part of the network. Thus, empathy served as a bridge between SDO and self-compassion which showed only weak connections. Self-compassion was distinctly split into highly intercorrelated 'positive' (Self-Kindness, Mindfulness, Common Humanity) and 'negative' (Self-Judgment, Over-Identification, Isolation) parts.

```{r node-labels}

node_labels <-
  data.frame(
    cbind(
      c("SDO",
        "IRI-EC/PT",
        "SCS-SK",
        "SCS-SJ",
        "SCS-CH",
        "SCS-I",
        "SCS-M",
        "SCS-OI"),
      c("Social Dominance Orientation",
        "Interpersonal Reactivity Index, Empathic Concern-Perspective Taking subscales composite",
        "Self-Compassion Scale, Self-Kindness subscale",
        "Self-Compassion Scale, Self-Judgment subscale",
        "Self-Compassion Scale, Common Humanity subscale",
        "Self-Compassion Scale, Isolation subscale",
        "Self-Compassion Scale, Mimdfulness subscale",
        "Self-Compassion Scale, Over-Identification subscale")
      )
    )

colnames(node_labels) <- c("Label", "Description")

kable(node_labels,
      caption = "Node labels and corresponding study variables")

```

```{r plot-networks, fig.cap = "Regularized partial correlation networks of pre- and post-COVID samples. Blue edges represent positive associations, red edges represent negative associations. Edge thickness and saturation indicate strength of association."}

par(mfrow = c(1,2))

net_joint_pre <- qgraph(net_joint$network[[1]],
                        layout = L,
                        title = "Pre-COVID sample",
                        theme="colorblind",
                        border.width=2,
                        vsize=10,
                        border.color='#555555',
                        label.color="#555555",
                        color="#EEEEEE",
                        labels = colnames(net_joint$network[[1]]))

net_joint_post <- qgraph(net_joint$network[[2]],
                        layout = L,
                        title = "Post-COVID sample",
                        theme = "colorblind",
                        border.width = 2,
                        vsize = 10,
                        border.color = '#555555',
                        label.color = "#555555",
                        color = "#EEEEEE",
                        labels = colnames(net_joint$network[[2]]))

```

Centrality indexes in pre- and post-COVID networks followed the same patterns (see Figure \@ref(fig:centrality-joint)). Betweenness showed larger discrepancies, but those could be due to the low stability of this index. Closeness was not sufficiently stable either (see details the next section), hence we do not discuss those two measures. SDO showed the lowest centrality indexes because it had a strong edge only with empathy and a low overall number of edges. Empathy demonstrated moderate centrality. Within the self-compassion part of the network, self-judgment had the strongest connections with other nodes, and mindfulness and common humanity had the highest potential positive influence in the network.

``` {r centrality-joint, fig.cap = "Netowork centrality indexes"}

cent_plot <- centralityPlot(list("pre-COVID" = net_joint$network[[1]],
                                 "post-COVID" = net_joint$network[[2]]),
               include = c("Strength", "Closeness", "ExpectedInfluence", "Betweenness"),
               print = FALSE)

cent_plot +
  labs(color = "Sample") +
  guides(color = guide_legend(reverse = TRUE))

# kable(centrality_auto(net_pre)$node.centrality[-1],
#       caption = "Centrality Estimates",
#       digits = 2)

```

#### Network Stability

To evaluate the accuracy of the edge-weights of the separately estimated pre- and post-COVID networks, we calculated their 95% bootstrapped confidence intervals (see Figures \@ref(fig:boot-edge-pre) and \@ref(fig:boot-edge-post)). In both networks, CIs of some of the edges overlapped substantially, indicating that comparison of those edge-weights should be avoided.

```{r boot-edge-pre, fig.cap = "Bootstrapped confidence intervals of estimated edge-weights of the pre-COVID network. The red line indicates the sample values and the gray area the bootstrapped CIs. The black line indicates the mean of the bootstrapped sample values. Each horizontal line represents one edge of the network, ordered from the edge with the highest edge-weight to the edge with the lowest edge-weight"}

# boot_edge_pre <- bootnet(net_pre, nCores = 8, nBoots = 5000)

load("data/clean/boot_edge_pre.Rda")

plot(boot_edge_pre, order = "sample")

# summary(boot_edge_pre) %>%
#   filter(type == "edge") %>%
#   select(node1, node2, sample, CIlower, CIupper) %>%
#   kable(digits= 2,
#       caption = "Bootstrapped confidence intervals of estimated edge-weights of the pre-COVID network")

```

```{r boot-edge-post, fig.cap = "Bootstrapped confidence intervals of estimated edge-weights of the post-COVID network. The red line indicates the sample values and the gray area the bootstrapped CIs. The black line indicates the mean of the bootstrapped sample values. Each horizontal line represents one edge of the network, ordered from the edge with the highest edge-weight to the edge with the lowest edge-weight"}

# boot_edge_post <- bootnet(net_post, nCores = 8, nBoots = 5000)

load("data/clean/boot_edge_post.Rda")

plot(boot_edge_post, order = "sample")

# summary(boot_edge_post) %>%
#   filter(type == "edge") %>%
#   select(node1, node2, sample, CIlower, CIupper) %>%
#   kable(digits= 2,
#       caption = "Bootstrapped confidence intervals of estimated edge-weights of the pre-COVID network")

```

Then, we evaluated stability of the centrality indexes of separately estimated networks (see Figures \@ref(fig:boot-centrality-pre) and \@ref(fig:boot-centrality-post)). Stability of strength and expected influence in both networks was good, with correlation stability (CS) coefficients of .75, the largest possible value. Stability of closeness in the pre-COVID network was acceptable, with CS coefficient of .60. However, in the post-COVID network CS coefficient of closeness was .44 which is below the recommended threshold of .50 (Epskamp et al., 2017). Stability of betweenness was poor, with CS coefficient of .21 in the both networks.

``` {r boot-centrality-pre, fig.cap = "Average correlations between centrality indexes of pre-COVID sample networks sampled with cases dropped and the original sample. Lines indicate the means and areas indicate the range from the 2.5th quantile to the 97.5th quantile"}

# boot_centrality_pre <- bootnet(net_pre, nCores = 8, nBoots = 5000,
#                     type = "case",
#                     statistics = c("edge", "strength", "expectedInfluence",
#                                    "closeness", "betweenness"))

load("data/clean/boot_centrality_pre.Rda")

# corStability(boot_centrality_pre)

plot(boot_centrality_pre, statistics = c("strength","closeness",
                                         "betweenness","expectedInfluence"))

```

``` {r boot-centrality-post, fig.cap = "Average correlations between centrality indexes of post-COVID sample networks sampled with cases dropped and the original sample. Lines indicate the means and areas indicate the range from the 2.5th quantile to the 97.5th quantile"}

# boot_centrality_post <- bootnet(net_post, nCores = 8, nBoots = 5000,
#                     type = "case",
#                     statistics = c("edge", "strength", "expectedInfluence",
#                                    "closeness", "betweenness"))

load("data/clean/boot_centrality_post.Rda")

# corStability(boot_centrality_post)

plot(boot_centrality_post, statistics = c("strength","closeness",
                                         "betweenness","expectedInfluence"))

```

#### Network Comparison

First, we calculated Spearman correlation of edge-weights in pre- and post-COVID networks. The coefficient was `r round(cor(c(net_pre$graph), c(net_post$graph), method = "spearman"), 2)`, indicating high similarity. Then, we used Network Comparison Test (NCT, van Borkulo et al., 2020) to compare edge-weights in the two networks. The results of the omnibus test indicated that none of the corresponding edge-weights significantly differed (*p* = .43). Global strength (the sum of all absolute edge-weights) in the two groups did not significantly differ either (*p* = .83), meaning that network connectivity was similar. Strength and expected influence of separate nodes did not significantly differ either (all *p*'s > .12). Together, those results point to considerable similarity between the two networks. Finally, we computed a cross-sample network by averaging all edge-weights. Figure \@ref(fig:average-network) depicts the network, and Figure \@ref(fig:average-centrality) shows strength and expected influence of its nodes.

```{r net-invariance, include = FALSE}

# net_comparison <- NCT(sc_net[sc_net$group == "PreCOVID",-9],
#                       sc_net[sc_net$group == "PostCOVID",-9],
#                       it = 5000,
#                       test.edges = TRUE,
#                       edges = "all",
#                       p.adjust.methods = "holm",
#                       test.centrality = TRUE,
#                       centrality = c("strength", "expectedInfluence"),
#                       progressbar = FALSE)

load("data/clean/nct_results.Rda")

net_comparison

```

```{r average-network, fig.cap = "Cross-sample network (N = 986) showing the average of pre- and post-COVID networks. Blue edges represent positive associations, red edges represent negative associations. Edge thickness and saturation indicate strength of association."}

net_average_wmat <- (getWmat(net_joint_pre) + getWmat(net_joint_post))/2

net_average <- qgraph(net_average_wmat,
                      layout = L,
                      title = "",
                      theme = "colorblind",
                      border.width = 2,
                      vsize = 10,
                      border.color = "#555555",
                      label.color = "#555555",
                      color="#EEEEEE",
                      labels = colnames(net_average_wmat))

```

```{r average-centrality, fig.cap = "Centrality indexes of the cross-sample network."}

cent_plot_average <- centralityPlot(net_average_wmat,
                                    include = c("Strength", "ExpectedInfluence"))

```


## Discussion

Since network models are largely data-driven and exploratory, it is important to ensure that the models replicate across different datasets (Fried et al., 2018).

## References

Borsboom, D., Fried, E. I., Epskamp, S., Waldorp, L. J., van Borkulo, C. D., van der Maas, H., & Cramer, A. (2017). False alarm? A comprehensive reanalysis of "Evidence that psychopathology symptom networks have limited replicability" by Forbes, Wright, Markon, and Krueger (2017). Journal of abnormal psychology, 126(7), 989–999. https://doi.org/10.1037/abn0000306

Costantini, G., Richetin, R., Preti, E., Casini, E., Epskamp, S., & Perugini, M. (2019). Stability and variability of personality networks. A tutorial on recent developments in network psychometrics Personality and Individual Differences, 136, 68-78. https://doi.org/10.1016/j.paid.2017.06.011

Danaher, P., Wang, P., Witten, D. M. (2014). The joint graphical lasso for inverse covariance estimation across multiple classes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(2), 373–397. https://doi.org/10.1111/rssb.12033

Epskamp, S., Borsboom, D., & Fried, E. I. (2018). Estimating psychological networks and their accuracy: a tutorial paper. Behavior Research Methods, 50, 195–212. https://doi.org/10.3758/s13428-017-0862-1

Epskamp, S., Cramer, A. O. J., Waldorp, L. J., Schmittmann, V. D., Borsboom, D. (2012). qgraph: Network visualizations of relationships in psychometric data. Journal of Statistical Software, 48(4), 1–18. http://dx.doi.org/10.18637/jss.v048.i04

Fried, E. I., Eidhof, M. B., Palic, S., Costantini, G., Huisman-van Dijk, H. M., Bockting, C. L. H., Engelhard, I., Armour, C., Nielsen, A. B. S., & Karstoft, K.-I. (2018). Replicability and Generalizability of Posttraumatic Stress Disorder (PTSD) Networks: A Cross-Cultural Multisite Study of PTSD Symptoms in Four Trauma Patient Samples. Clinical Psychological Science, 6(3), 335–351. https://doi.org/10.1177/2167702617745092

Greszki, R., Meyer, M., & Schoen, H. (2015). Exploring the effects of removing “too fast” responses and respondents from web surveys. Public Opinion Quarterly, 79(2), 471-503. https://doi.org/10.1093/poq/nfu058

Lauritzen, S. L. (1996). Graphical Models. Clarendon Press.

Rhemtulla, M., Fried, E. I., Aggen, S. H., Tuerlinckx, F., Kendler, K. S., & Borsboom, D. (2016). Network analysis of substance abuse and dependence symptoms. Drug and Alcohol Dependence, 161, 230–237. https://doi.org/10.1016/j.drugalcdep.2016.02.005

Tabachnick, B. G. & Fidell, L. S.(2013). Using multivariate statistics. 6th ed. Pearson.

van Borkulo, C. D., Waldorp, L. J., Boschloo, L., Kossakowski, J., Tio, P., L., Schoevers, R.A.,
& Borsboom, D. (2020). Comparing network structures on three aspects:: A permutation test (Working Paper). https://doi.org/10.13140/RG.2.2.29455.38569